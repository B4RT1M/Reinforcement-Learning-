{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "A2C.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "sx7x0m4zgR1N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#mount drive \n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tljv5RKLgX9-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install gym\n",
        "!apt-get install python-opengl -y\n",
        "!apt install xvfb -y\n",
        "!pip install gym[atari]\n",
        "!pip install pyvirtualdisplay\n",
        "!conda install piglet\n",
        "!pip install pystan\n",
        "!conda install swig\n",
        "!pip install box2d-py\n",
        "!pip install gym[Box_2D]\n",
        "\n",
        "!pip3 install pybullet --upgrade"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O-Q8s-M5gawM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5rbJrA9IgiMt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This code creates a virtual display to draw game images on. \n",
        "# If you are running locally, just ignore it\n",
        "import os\n",
        "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\"))==0:\n",
        "    !bash ../xvfb start\n",
        "    %env DISPLAY=:1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "notlKBK8gotS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.distributions import MultivariateNormal\n",
        "import gym\n",
        "import numpy as np\n",
        "from IPython.display import clear_output\n",
        "import matplotlib.pyplot as plt\n",
        "get_ipython().run_line_magic('matplotlib', 'inline')\n",
        "import pybullet_envs\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def plot(avg_rewards, value_losses, action_losses, log_interval):\n",
        "    '''        For monitoring the training process    '''\n",
        "    clear_output(True)\n",
        "    # plt.figure(figsize=(7,7)) \n",
        "    # plt.title(' reward: %s' % (np.mean(avg_rewards[-10:])))\n",
        "    # plt.plot(avg_rewards)\n",
        "    # plt.show()\n",
        "\n",
        "    \n",
        "    plt.figure(figsize=(17,5))\n",
        "    plt.subplot(1,3,1)\n",
        "    plt.title('Episodic reward: %s' % (np.mean(avg_rewards[-log_interval:])))\n",
        "    plt.scatter(np.linspace(0,len(avg_rewards)-1,len(avg_rewards)),avg_rewards, s=1)\n",
        "\n",
        "    plt.subplot(1,3,2)\n",
        "    plt.title('value loss: %s' % ( np.mean(value_losses[-log_interval:])))\n",
        "    plt.scatter(np.linspace(0,len(value_losses)-1,len(value_losses)),value_losses, s=1)\n",
        "    \n",
        "    plt.subplot(1,3,3)\n",
        "    plt.title('action loss: %s' % ( np.mean(action_losses[-log_interval:])))\n",
        "    plt.scatter(np.linspace(0,len(action_losses)-1,len(action_losses)),action_losses, s=1)\n",
        "    plt.show()\n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "class Memory:\n",
        "    def __init__(self):\n",
        "        self.actions = []\n",
        "        self.states = []\n",
        "        self.next_states = []\n",
        "        self.logprobs = []\n",
        "        self.rewards = []\n",
        "        self.dones = []\n",
        "        self.bootstrapes = []\n",
        "    \n",
        "    def clear_memory(self):\n",
        "        del self.actions[:]\n",
        "        del self.states[:]\n",
        "        del self.logprobs[:]\n",
        "        del self.rewards[:]\n",
        "        del self.dones[:]\n",
        "        del self.bootstrapes[:]\n",
        "        del self.next_states[:]\n",
        "        \n",
        "    def insert(self, state,action,next_state,reward,done,bootstrap):\n",
        "        self.states.append(state)\n",
        "        self.actions.append(action)\n",
        "        self.next_states.append(next_state)\n",
        "        self.rewards.append(reward)\n",
        "        self.dones.append(done)\n",
        "        self.bootstrapes.append(bootstrap)\n",
        "        \n",
        "\n",
        "class Critic(nn.Module):\n",
        "    def __init__(self, state_dim):\n",
        "        super(Critic, self).__init__()\n",
        "        '''       Create an Actor network with 2 hidden layers.\n",
        "                  Each hidden layer with 64 neurons and using LeakyReLU(slope = 0.2) as activation.\n",
        "                  The output of the network is the approximated value estimate for the state'''\n",
        "        self.critic = nn.Sequential(\n",
        "                nn.Linear(state_dim,64),\n",
        "                nn.LeakyReLU(negative_slope = 0.2), \n",
        "                nn.Linear(64,64),\n",
        "                nn.LeakyReLU(0.2),\n",
        "                nn.Linear(64,1)\n",
        "                )\n",
        "\n",
        "    def forward(self):\n",
        "        raise NotImplementedError\n",
        "        \n",
        "    def evaluate_state_value_no_gradient(self, state):\n",
        "        with torch.no_grad():\n",
        "            value = self.critic(state).detach()\n",
        "        return value\n",
        "    \n",
        "    \n",
        "    def evaluate_state_value(self, state):\n",
        "        value = self.critic(state)            \n",
        "        return value\n",
        "\n",
        "    \n",
        "    \n",
        "class Actor(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        super(Actor, self).__init__()\n",
        "        self.log_std_min = -5\n",
        "        self.log_std_max = 2\n",
        "        '''To Do: Create an Actor network with 2 hidden layers.\n",
        "                  Each hidden layer with 64 neurons and using LeakyReLU(slope = 0.2) as activation.\n",
        "                  The output of the network should be:\n",
        "                  (a) the mean for each action dimension :mu\n",
        "                  (b) the ln(variance) for each action dimension :log_var\n",
        "                  The output is therefore 2* action_dim\n",
        "           Note : The final out action must be bounded in an allowable range defined by the environment. i.e. [-1,1]\n",
        "                  This is achieved by using nn.Tanh() for the output layer.         \n",
        "        '''\n",
        "\n",
        "\n",
        "\n",
        "        \n",
        "    def forward(self, state):\n",
        "        ''' To Do: Input : state (Tensor)\n",
        "            Return : mu and var (Tensor)\n",
        "            Note : 'log_var' must be exponentiated to get 'var'\n",
        "        '''\n",
        "        mu = ...\n",
        "        log_var = ...\n",
        "        # Clamp the log_var to make the value range of log_var to be [self.log_std_min, self.log_std_max] to avoid too deterministic actions or too large exploration.\n",
        "        # Useful function : torch.clamp()\n",
        "        log_var = torch.clamp(log_var,self.log_std_min, self.log_std_max)\n",
        "        var = torch.exp(log_var([0,math.log(2.)])) # exponentiate  log_var\n",
        "        return mu, var\n",
        "        \n",
        "    def act(self, state):\n",
        "        ''' To Do:\n",
        "            Refer to https://pytorch.org/docs/stable/distributions.html\n",
        "            (1) first pass the state S to NN to get action_mean, and action_var\n",
        "            (2) Create a torch.distribution.MultivariateNormal which takes the mean and variance, variance stays the same for all states. Useful functions: torch.diag_embed.diag(),\n",
        "            (3) sample an action according to the normal distribution in step 2 ,  torch.distributions.sample()\n",
        "            Return action (in tensor), Note : .to(device) is required to make sure tensors are all on GPU or CPU.\n",
        "            useful function: torch.diag_embed.diag(), MultivariateNormal(), .sample() \n",
        "            Input : state  (Tensor, [1, state_dim])\n",
        "            Return: action (Tensor, [1, action_dim])\n",
        "        '''\n",
        "        with torch.no_grad():\n",
        "            action_mean, action_var = self.forward(state)\n",
        "        cov_mat = ...  # define the covaraince matrix from action_var, which is of shape [ 1, action_dim, action_dim] \n",
        "        dist = ... # initiate an instance of  multivariate Gaussian distribution\n",
        "        action = dist.sample().to(device) # sample an action from dist, which is of shape [1, action_dim]\n",
        "        return action.detach()\n",
        "\n",
        "    def act_test(self, state):\n",
        "        ''' For testing phase, we would like to have deterministic action \n",
        "            by taking the mean of the Gaussian Distribution.\n",
        "            Return test_action (in tensor)\n",
        "            Input : state  (Tensor, [1, state_dim])\n",
        "            Return: action (Tensor, [1, action_dim])\n",
        "        '''       \n",
        "        with torch.no_grad():\n",
        "            action_mean,_ = self.forward(state)\n",
        "        return action_mean.detach()\n",
        "    \n",
        "    def evaluate(self, state, action):   \n",
        "        ''' To Do: Compute the action_logprobs and entropy in batch mode (for all samples in memory)        \n",
        "            Useful function in pytorch : torch.diag_embed(), distribution.log_prob(), distribution.entropy()  \n",
        "            see https://pytorch.org/docs/stable/distributions.html            \n",
        "            Input : state (Tensor, [#rollout, state_dim]), action (Tensor, [#rollout,action_dim])\n",
        "            Return: action_logprobs (1-D tensor)--> torch.size([rollout]) , dist_entropy (1-D tensor)--> torch.size([rollout])\n",
        "            The first dimension of cov_mat takes the form of an diagonalized matrix [action_dim_1_var, 0...\n",
        "                                                                                     0, action_dim_2_var, 0,...,\n",
        "                                                                                     ...\n",
        "                                                                                     0 ......, action_dim_N_var], \n",
        "            where off-diagonal elements are 0, assuming independence among each action dimension.\n",
        "        '''  \n",
        "        action_mean, action_var = self.forward(state)        \n",
        "        action_var = action_var.expand_as(action_mean)\n",
        "        cov_mat = ...      #3-D Tensor [#rollout, action_dim, action_dim] , where the action var is on\n",
        "        dist = ...    \n",
        "        if len(action.shape) == 1:\n",
        "            action = action.unsqueeze(1)\n",
        "        action_logprobs = ...\n",
        "        dist_entropy = ...  \n",
        "        return action_logprobs , dist_entropy\n",
        "\n",
        "\n",
        "\n",
        "class A2C:\n",
        "    def __init__(self, state_dim, action_dim, lr, betas, gamma):\n",
        "        self.lr = lr\n",
        "        self.betas = betas\n",
        "        self.gamma = gamma\n",
        "        self.actor = Actor(state_dim, action_dim).to(device)\n",
        "        self.critic = Critic(state_dim).to(device)   \n",
        "        self.optimizer_actor = torch.optim.Adam(self.actor.parameters(), lr=lr)\n",
        "        self.optimizer_critic = torch.optim.Adam(self.critic.parameters(), lr=lr*5)\n",
        "        self.MseLoss = nn.MSELoss()\n",
        "    \n",
        "    def select_action(self, state):\n",
        "        state = torch.FloatTensor(state.reshape(1, -1)).to(device)\n",
        "        return self.actor.act(state).cpu().data.numpy().flatten()\n",
        "\n",
        "\n",
        "    def select_test_action(self, state):\n",
        "        state = torch.FloatTensor(state.reshape(1, -1)).to(device)\n",
        "        return self.actor.act_test(state).cpu().data.numpy().flatten()\n",
        "    \n",
        "        \n",
        "    \n",
        "    def update(self, memory):\n",
        "        '''This part is almost the same as A2C-single_thread-discrete.py. Here we do Advantage Actor-critic update'''\n",
        "        # convert list to tensor\n",
        "        states = torch.FloatTensor(memory.states).to(device)\n",
        "        actions = torch.FloatTensor(memory.actions).to(device)\n",
        "\n",
        "        # --------------------compute value loss----------------\n",
        "        # IMPORTANT: Compute the Monte-Carlo Returns as the update target for all V(S_t)        \n",
        "        state_values = self.critic.evaluate_state_value(states)         \n",
        "        \n",
        "        # IMPORTANT: we will compute advantage based on the returns, the advantage we use here is the fourth formulae in Page 22 (the one with no bias and high variance) in http://rail.eecs.berkeley.edu/deeprlcourse/static/slides/lec-6.pdf.\n",
        "        returns = [] # Target value for V(S_t), where Monte-Carlo returns are used.\n",
        " \n",
        "        next_states = torch.FloatTensor(memory.next_states).to(device)#.unsqueeze(0) \n",
        "        next_state_values = self.critic.evaluate_state_value_no_gradient(next_states).squeeze().cpu().numpy()\n",
        "        # initialize the td_target with last element of next_state_values to allow for bootstrapping in case of pseudo termination(max episodic length reached)\n",
        "        td_target = next_state_values[-1]\n",
        "        # ----------Start computing returns------------\n",
        "        ct = -1\n",
        "        for reward, done, bootstrap in zip(reversed(memory.rewards), reversed(memory.dones), reversed(memory.bootstrapes)):\n",
        "            # returns = [td_target(S_0),td_target(S_1)...,td_target(S_T-1), # for the first episode\n",
        "            #            td_target(S_0),td_target(S_1)...,td_target(S_T-1), # for the second episode\n",
        "            #            ,....,\n",
        "            #            td_target(S_0),td_target(S_1)...,td_target(S_T-1), # for the last episode]\n",
        "            # returns is computed in reversed order!! First compute td_target(S_T-1) from the last episode,\n",
        "            # then to  td_target(S_T-2) from the last epi,....td(S_0) of the last epi, td(S_T-1) of the LAST SECOND epi, td(S_T-2).... until the first\n",
        "            if done==True and bootstrap==False: # \n",
        "                td_target = 0 # To think\n",
        "            elif bootstrap==True: # Pseudo termination due to max episodic length being reached\n",
        "                #v_pi(s_t) = 1/N sum_{i=1}^{N}sum_{t prime = t}^{T} discount^{t prime - t - 1} r(s_t prime,a_t prime) + discount V_pi (s_T+1,a_T+1)\n",
        "                td_target = next_state_values[ct] # to think,   \n",
        "            td_target = (self.gamma) * td_target + reward #a recursive form to compute the td_target as the return, 'td_target' should appear on the right part of the equation.\n",
        "            # Insert the td_target into the first element of the list 'returns', while maintaning the existing elements in the list.\n",
        "            returns.insert(0,td_target)\n",
        "            ct -= 1\n",
        "            \n",
        "        returns = torch.FloatTensor(returns).to(device)\n",
        "\n",
        "        \n",
        "        #-----------Update value estimate-------------\n",
        "        value_loss = # To do \n",
        "        self.optimizer_critic.zero_grad()\n",
        "        value_loss.backward()\n",
        "        self.optimizer_critic.step()\n",
        "  \n",
        "        #------------------ update action loss---------------- \n",
        "        # compute advantages\n",
        "        advantages = td_target - state_values # as mentioned above we Monte Carlo returns to compute advantages, See http://rail.eecs.berkeley.edu/deeprlcourse/static/slides/lec-6.pdf ,       \n",
        "        logprobs, dist_entropy = self.actor.evaluate(states, actions)        \n",
        "        action_loss = -(log_probs * advantage.detach()).mean()   \n",
        "        # Encourage the agent to explore by maximizing the entropy\n",
        "        action_loss =  action_loss # TO think : plus the entropy or minus the entropy   0.01 * dist_entropy.mean() \n",
        "        self.optimizer_actor.zero_grad()\n",
        "        action_loss.backward()\n",
        "        self.optimizer_actor.step()       \n",
        "        # print('value loss: {}, action loss: {}'.format(value_loss, action_loss))  \n",
        "        return value_loss.cpu().data.numpy().flatten(), action_loss.cpu().data.numpy().flatten()\n",
        "\n",
        "\n",
        "def main():\n",
        "    ############## Hyperparameters ##############\n",
        "    env_name = 'InvertedPendulumBulletEnv-v0'  \n",
        "    # creating environment\n",
        "    env = gym.make(env_name)\n",
        "    env_test = gym.make(env_name)\n",
        "    state_dim = env.observation_space.shape[0]\n",
        "    action_dim = env.action_space.shape[0]\n",
        "    render = False\n",
        "    solved_reward = np.inf         # stop training if avg_reward > solved_reward\n",
        "    log_interval = 1           # print avg reward in the interval\n",
        "    max_iterations = 10000        # max training episodes\n",
        "    max_timesteps = env._max_episode_steps        # max timesteps in one episode\n",
        "    print(env._max_episode_steps)\n",
        "    update_every_N_complete_episode = 8      # IMPORTANT : update policy every _N_complete_episode, for A2C, this is set to be the number of parallel agents.\n",
        "    gamma = 0.99                # discount factor\n",
        "    lr = 0.001                 # parameters for Adam optimizer\n",
        "    betas = (0.9, 0.999)\n",
        "    # Note: Boostraping mode affects (1) how the MC-returns are computed in A2C.update() \n",
        "    # (2) If enabled, only in case of the pseudo termination (reaching maximal episode instead of terminal states), done is set as False, Bootstrap flag is set true.\n",
        "    # For A2C, we set bootstrapping mode to be True\n",
        "    allow_bootstrapping_mode = True    \n",
        "    random_seed = None\n",
        "    #############################################\n",
        "   \n",
        "    \n",
        "    if random_seed:\n",
        "        print(\"Random Seed: {}\".format(random_seed))\n",
        "        torch.manual_seed(random_seed)\n",
        "        env.seed(random_seed)\n",
        "        np.random.seed(random_seed)\n",
        "    \n",
        "    memory = Memory()\n",
        "    a2c = A2C(state_dim, action_dim, lr, betas, gamma)\n",
        "    print(lr,betas)\n",
        "    \n",
        "    # logging variables\n",
        "    episodic_reward = 0\n",
        "    time_step = 0 # Count of episodic length within each update iteration, of different definition from previous versions\n",
        "    done_count_in_an_itr = 0\n",
        "    avg_rewards, value_losses, action_losses = [] ,[], []\n",
        "    \n",
        "    # training loop, one iteration means collecting (N = 8) complete episodes and then do the update for both actor and critic.\n",
        "    for i_iter in range(1, max_iterations+1):\n",
        "        state = env.reset()            \n",
        "        episodic_reward, avg_reward = 0, 0\n",
        "        done_count_in_an_itr, time_step = 0 , 0\n",
        "        # -----------------Testing_phase-----------------\n",
        "        if i_iter % 10 == 0:                  \n",
        "            test_reward = 0\n",
        "            test_state = env_test.reset()            \n",
        "            test_done = False\n",
        "            print('-----------starting test-----------')\n",
        "            for i in range(max_timesteps):\n",
        "                action = a2c.select_test_action(test_state)\n",
        "                # env_test.render()\n",
        "                test_state, reward, test_done, _ = env_test.step(action)\n",
        "                test_reward += reward\n",
        "                if test_done == True:\n",
        "                    break\n",
        "            print('Test reward : {}'.format(test_reward))\n",
        "       \n",
        "        \n",
        "        while done_count_in_an_itr < update_every_N_complete_episode:\n",
        "            time_step +=1\n",
        "            # Run policy\n",
        "            action = a2c.select_action(state)\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "\n",
        "            # Saving reward and dones to the temporary buffer: \n",
        "            if (time_step == env._max_episode_steps) and (done == True):     \n",
        "                if allow_bootstrapping_mode:\n",
        "                    # Enable bootstrapping for the last experience reaching maximal episodic length\n",
        "                    memory.insert(state, action, next_state, reward, False, True)\n",
        "                else:\n",
        "                    # Disable bootstrapping\n",
        "                    memory.insert(state, action, next_state, reward, done, False)\n",
        "                time_step = 0\n",
        "            else:\n",
        "                memory.insert(state, action, next_state, reward, done, False)\n",
        "            \n",
        "            # update after collecting N complete epsidoes, rollout size can differ for each iteration\n",
        "            if (done_count_in_an_itr == update_every_N_complete_episode - 1) and (done):\n",
        "                value_loss, action_loss = a2c.update(memory)\n",
        "                # Clear the memory when update is done!\n",
        "                memory.clear_memory()\n",
        "                value_losses.append(value_loss[0])\n",
        "                action_losses.append(action_loss[0])\n",
        "            \n",
        "            state = next_state    \n",
        "            episodic_reward += reward\n",
        "            \n",
        "            if done:  \n",
        "                state = env.reset()\n",
        "                done_count_in_an_itr += 1\n",
        "                avg_reward += (1/done_count_in_an_itr) * (episodic_reward-avg_reward)\n",
        "                episodic_reward = 0\n",
        "                time_step = 0\n",
        "                avg_rewards.append(avg_reward)\n",
        "\n",
        "        \n",
        "        # stop training if avg_reward > solved_reward\n",
        "        if avg_reward > solved_reward:\n",
        "            print(\"########## Solved! ##########\")\n",
        "            # You can save network here for animation purpose\n",
        "            break\n",
        "        \n",
        "            \n",
        "        # logging\n",
        "        if i_iter % log_interval == 0:\n",
        "            print('Iteration {} \\t Avg reward: {}'.format(i_iter, avg_reward))\n",
        "            plot(avg_rewards, value_losses, action_losses, log_interval)\n",
        "            \n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}